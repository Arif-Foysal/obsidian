#FYDP #project 


>[!Ideas]
>- Suggesting the most likely condition, with the other conditions that also might occur to the patient.


## Speech

Good afternoon everyone, today we are going to present our project idea which is **Computer Vision Based Medical Diagnosis System** designed to assist doctors in detecting and diagnosing diseases more accurately and efficiently.

Before we dive into the details, Take a look at this X-ray. At first glance, it might look normal, right? But what if I told you there’s actually a fracture somewhere in this image? 


Well it turns out that the pink box here highlights the fracture. 





## Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/3PbEgLw6lJ0?si=BkUuyWVFoZmiqkcz" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Transcript







## Video
<iframe width="560" height="315" src="https://www.youtube.com/embed/Et5HC8SR0BA?si=__-vSN0V_W15GHkL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
Transcriber: Elisabeth Buffard
Reviewer: Anna Sobota Take a look at this x-ray here. I’m going to give you a medical pop quiz. Does anything look out of place 
with this x-ray that you can see? What if I told you there was actually
a fracture somewhere in this x-ray? Would you believe me? Well, it turns out that is the case
as shown by this pink box here. That little crinkle there that you see is actually what’s called
a Taurus fracture. See, this is a child’s hand, this x-ray, and children’s bones are more malleable than adult bones. So sometimes when they break their wrist, it’s more of a dent or a bend 
than it is an actual clean break. Now, you might not know 
to look for things like this, but I only happen to know 
because I went to medical school. I’m the person that built this AI. My name is Logan Nye, I am  a research physician
at Harvard Medical School, where I develop 
intelligent systems for health care or in other words, 
I’m a physician who codes. And that makes me unique 
and in a couple of ways. One, it makes it so that I’m probably the biggest nerd
that you’ve ever met in your life. That’s quite the overlap. But also it gives me
a unique vantage point for creating artificial intelligence
for medical diagnostics and health care. Usually when you develop 
artificial intelligence, you have developers and you have 
domain experts and there’s some crosstalk and they develop these
sorts of algorithms. But I want to show you some insights
as to why it’s helped me to be someone
with scientific domain knowledge that just learns how to develop 
artificial intelligence directly. So to do so, I’m going to share 
some examples from my research at MGH from the past year. Here we have two images of histology, meaning the cells under a microscope. They look pretty similar, right? These are both showing cartilage tissue,
but not just any cartilage tissue. They’re both tumors. On the left we have enchondroma,
which is a benign condition. It’s just a clump of cells that you don’t need 
any particular medical care for. You just keep an eye on it
for the rest of your life. But on the right-hand side, 
we have low-grade chondrosarcoma. It looks almost identical, but it’s vastly different results. Low-grade chondrosarcoma
is very aggressive and needs to be treated aggressively. It requires surgery, often amputation and cautery of the wound
that you scrape out. So obviously, it's important to know
the difference between these two. But historically, we’ve been about 50% reliable
in telling them apart on histology. I know about this because I’m in medicine and it’s a widely known problem. But as I also happen to know how to build
artificial intelligence systems, I realize that this is great
for image classification, a computer vision problem. And so over the course of an afternoon, we built a computer vision program that took our diagnostic accuracy
between these two different tumor types from around 50% to 75 to 90%. It was a long-standing problem
for several decades but if we apply AI and know
how to apply it in a clinical context, we can solve those long-standing problems
in a short course of time. And there’s other applications as well. Take these knee MRIs. These are showing different ACLs
or anterior cruciate ligament that get torn. Using computer vision, we can automatically find 
where and when they’re torn. On the left-hand side
we have a healthy ACL, in the middle
a completely ruptured ACL, on the right the AI systems
that we develop are able to tell that
it’s only partially torn. This is another way that computer vision
can be applied within health care. That’s just the tip of the iceberg. We have scaphoid fractures. We’re able to detect lumbar disc 
herniations when people have back pain and need to localize where their nervous
system is being pressed on. We’re able to get fractures in arm areas, foot and ankle, hips, and we’re even able to quantify
osteoarthritis in knees to tell people when they should
be considering getting a knee replacement. All this is possible because we apply
artificial intelligence from a clinical standpoint. In our lab, we’ve also worked
on cancer diagnostic systems similar to the enchondroma problem, but applied to over 30 different 
other soft tissue tumors. And not just soft tissues,
but 15 or more bone tumors as well. We’re continuing to expand experiment
with these different applications in the clinical context. Again made possible
because we know what to look for and thus how to build AI. And in some cases,
when you have rare diseases, you lack the data to create these useful artificial
intelligence programs. Enchondroma is an example of this,
that first tumor that we looked at. But thanks to generative AI,
we can bridge that gap and the scarcity
of medical data that exists. This histology image of enchondroma
actually doesn’t exist. It was generated by AI, but because we know that
that’s a gap that exists in clinical care, we can leverage AI to fill those needs. Now, you might ask why is this important? I can tell you why
it’s personally important and why I became interested 
in developing artificial intelligence. During medical school,
I had the opportunity to participate in global health trips. One of them was to sub-Saharan Africa. The people there are incredible
and do awesome things with what they have, but there’s no denying
that they lack resources. We drived all the way out into the bush and we would spend all day working with the types of supplies that you’d find
at OfficeMax or Home Depot to care for patients. But one thing that I noticed was that the medical staff used iPads 
to record all their patient data. They had access to smartphones. And again, when I went to the Himalayas, we treated all sorts of patients
in far reaches across the Himalayan mountain range and they lacked those resources,
but they all had smartphones. So anybody that has a smartphone is able to access
artificial intelligence systems and thus get proper care. And so that’s what we’re working on. We’re working on apps and APIs that we can open source,
make available to anybody anywhere. If you have Internet access
and if you have a smart device, we can democratize our medical expertise
to those who most need it. Preventing the need to travel 
for days or even weeks to get the care that they need. And so, I would encourage
other people that are domain experts, whether it be in medicine,
law, education, whatever, to try to find ways
to use artificial intelligence to amplify your expertise 
and make it available to other people. Who knows, it might just save lives. Thank you. (Applause).

## video
<iframe width="560" height="315" src="https://www.youtube.com/embed/uvqDTbusdUU?si=3rgDsbtTbN__Kjwc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
#### **Transcript:**
 Artificial intelligence has often been depicted as villain robots ready to take over the world.  
But I’m here to tell you that AI can actually save lives and improve health care for millions of patients around the world.  
AI is helping us personalize the delivery of care, make hospitals more efficient, and improve access to health care by providing accurate decision-making tools.  
AI is the process of educating a computer model using complex and large data sets.  
The model learns from this data in a training process to build its ability to make decisions or predict outcomes when presented with new data.  
We are talking about having access to a computer model that knows, based on the experience of thousands of other patients, whether a treatment is likely to work and what works best for that patient based on their individual conditions.  
No two of you in this room or, in fact, anywhere in the world are alike.  
But AI models are helping our doctors learn from patients with similar conditions or even similar genetic information and make highly informed decisions about their diagnosis and their treatment options.  
I want to talk about how we are starting to use AI for delivering care to cancer patients.  
Cancer diagnosis can be immensely complicated, both for the doctors in making decisions about diagnosing a primary or secondary cancer, as well as for the patients, in understanding the risks and success rates of the treatment options.  
But we are developing AI models that can help streamline this process by taking information from a number of sources.  
This involves feeding an AI model data from the patient’s blood tests, X-ray images of the suspected lesions, as well as genetic information from a tissue biopsy.  
The trained AI model can rapidly consolidate this information and provide highly accurate predictions of the patient's diagnosis, treatment options most likely to succeed, as well as the prognosis.  
Let’s talk about Peter, who is a cancer patient.  
He’s gone through comprehensive clinical assessment, imaging and various other diagnostic workups, but not even the best doctors in town can tell him where his cancer primary site is, meaning he can’t get a treatment specific for his cancer and his chances of surviving another five years is less than ten percent.  
But our team right here in Brisbane has developed a tool using AI and patients’ genetic information that can accurately identify the cancer primary site of Peter, and empower doctors to give Peter a treatment that we know is going to work for him.  
These type of models can be expanded exponentially to predict accurate health care.  
This means using an AI model to understand whether a certain population is more susceptible to a certain disease and whether they would respond more favorablyto certain health care interventions.  
AI is giving us the ability to have a much more refinedand detailed understanding of human health than we’ve ever had before.  
But there is a catch to the immense promise of AI being implemented into routine clinical practice.  
Our existing regulation frameworks aren’t designed for AI software intended for diagnosing, treating or managing the disease, also known as AI-based software as a medical device.  
They are designed for physical medical devices, like surgical implants, or most software that have the same output every time that the patient or clinicians are using them.  
Traditional software are static, in a sense that the developers release a version of a software and, no matter how many times you use it, it would always have the same output for the same data.  
On the other hand, AI software behaves completely differently to most software in health care because of the intrinsic ability to learn and evolve over time, ideally becoming more intelligent as suited to the environment that they’re being used at.  
Our existing regulation frameworks rely on the static and reproducible nature of this software to prove that they are safe to be implemented into routine clinical practice.  
So, our regulatory authorities’ solution has been to lock the learning potential of these algorithms before they are implemented into clinical practice.  
This means that the model can no longer learn from its environment and new data, which limits its potential to improve its functionality or its accuracy, you know, the whole point of AI.  
And, at times, this can even be harmful for the patientsbecause the AI model is no longer trained on the most up-to-date data and can potentially lead to a wrong diagnosis.  
But the good news is that there are emerging regulation frameworks being proposed that, if implemented right, can be a game changer.  
Our regulatory authorities are proposing using more transparent reporting mechanism so that the developers can disclose how their models would learn and evolve over time.  
And this will be combined with ongoing and real-time monitoring to make sure that the predicted changes actually occur and that the software is adaptive to make much more accurate predictions and improve health care outcomes.  
We also need to make sure that the training data used for these algorithms are representative of the entire human population.  
Let’s look at a mobile-based diagnostic software that we are developing right here in Brisbane that uses AI to detect skin cancer from the images that you’ve taken on your iPhone.  
If this model has been trained on a predominantly Caucasian population, how well do you think it would doon an African American or an Asian patient?  
Our AI developers have a huge responsibility to make sure that data bias doesn’t exist and that their models are trained on diverse and robust data sets, representative of the entire population, you know, not just white males.  
But at times, we understand that this is not entirely possible.  
Skin cancer does, in fact, disproportionately affect the Caucasian population because of the genetic differences, and, as a result, there are much larger data sets available for those patients.  
But this means that we need to build in a functionality in our AI models that, for low confidence results, for an Asian patient, for example, the model is capable of saying “I don’t know” or that “This is my best guess based on a skewed training population.” But, unfortunately, this functionality doesn’t exist yet, and it’s urgently needed to be mandated by our regulators.  
To successfully implement AI in health care, we need to establish new regulatory frameworks in consultation with AI developers, health care practitioners, policy advisers, as well as the patients themselves, to bring the best out of AI.  
Improve the regulatory frameworks can make sure that diverse and robust tools are developed that are compliant and adaptive and can serve the whole population equally.  
If we get this right, we can transform the delivery of health care where we are promoting personalized health and well-being advice.  
I’m excited to be at the forefront of translating this amazing technology into health care and use this to help millions of lives around the world.


## Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/7ZsyYCZB3Nw?si=iku0J4F0SP82JE1n" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
#### Transcript
Transcriber: D J
Reviewer: Manlin Fang Ten years ago, I found myself
in the office of a singular individual, 
Dr. John Perlin. At the time, he was 
the chief medical officer for a hospital company 
called HCA healthcare, where I was applying for a
job as a data scientist. and I said to him, 
Dr. Perlin, what is it that you’d have me, 
a data guy, do for you? And he proceeded to tell me 
about a disease called sepsis. It’s a systemic bloodstream infection, 
that if identified early, can be very readily treated with about
$0.25 worth of antibiotics and fluid. But if it goes unrecognized,
Spreads through the body, leading to systemic inflammation, organ
failure, and all too frequently, death. And Doctor Perlin 
proceeded to tell me at length about the etiology, biochemistry,
pharmacology of this disease. And I understood none of
what he was telling me. And I said, John, I really don’t understand, 
I’m not that kind of doctor. I'm a doctor of numbers. And
he said, I understand. I tell you all this to convince
you that as clinicians, we’ve tried everything we can do
in the fight against sepsis, and we are still losing. Our care simply does not arrive 
in time to be relevant. We need an early warning of sepsis. We believe it’s in the data, and that you and your fellow data scientists
can find it. Well, what Dr. Perlin didn’t know, was that 20 years prior 
to that conversation, sepsis had killed my father. He was 58 and I was a teenager, so I found
myself in a room opposite a man, telling me that for want of $0.25 worth
of drugs, my dad might still be alive. And moreover, that I could play a part in preventing other families going
through what I did and my family did. I entered that office seeking a job, 
and I left carrying a mission. But what little did I know. 
You see, healthcare itself is sick. Something is very, very
wrong in health care. You see, we've never had more
well-educated, more dedicated, more specialized physicians. We’ve never had instruments, and labs, and diagnostic tools that
are more precise. We’ve never had therapies and
drugs that are more potent, and can render what 
was once a death sentence, as a mere inconvenience or meme. And yet, despite all of this, healthcare is failing on 
any measure that we care about, be it clinical quality,
patient experience, and most of all, the $4.5 trillion bill that healthcare
levies on America every year. Things are getting worse. But why? People
tell me, “Edmund, it’s complicated.” No, It’s complexity. In healthcare, we 
have never slain the demon of complexity. You see, in other parts of the economy,
like technology or consumer goods, there are patterns and modes of thought
like standardization, automation, playbooks that enable us 
to simplify what we’re doing, by doing the same thing 
over and over again. My Apple Watch is a great example of this. A tremendously complicated device, and yet one of millions
of identical devices. That standardization enables Apple
to make the device at low cost and high quality. But in healthcare,
we are the product, and there's no such thing
as a standard human. Heck, I don't even know what size
my shoe is half the time. And so everything we’ve learned
and applied in the rest of the economy, about simplification, 
doesn’t work in healthcare, and the cruel irony, is that those new 
specializations of physicians, those new instruments, 
those new therapies, while individually good, combine to increase complexity, 
and so make things worse. It’s a pure torture. What we need, is a new mode 
of thought to simplify. We need new technology. And that technology is AI. What is it? An AI is simply a machine that thinks
like a human, but at scale. The first generation of AIs were AIs 
that could do logic, as you and I do. There were enormous branches,
trees of if-then-else statements, inductive and abductive logic. And they worked. But they were incredibly intricate,
expensive to produce, and even more expensive to maintain, and so they fell into disuse. They were the first and the
second AI boom bust cycle. The second generation of AIs
were AIs that can do mathematics and see patterns as we do Share with data, the AI through
machine learning, can infer a model which makes 
predictions about the future. You know, we used to joke 
about the weatherman and how useless and unreliable they were, 
and we don’t anymore, because aided by AI, they are really damn good, they are. The third and current generation of AIs are machines that understand 
language as we do. Catchers
gave great examples of that. These are machines that can
read and write, listen, and speak, in all of our human languages, 
but at scale and,  as a society, we haven’t yet figured out exactly 
what we’re going to do with all of this but we know it's big. One of the leading
proponents is a company called OpenAI, last valued at $80 billion on revenues
of one and losses of 5 billion. And what is it? It's a text box. But it’s a text box, critically, that
contains the whole internet. You see, OpenAI has a machine
that has read the internet, and will answer 
all of your questions about it. And that a text box containing
the internet should be worth an unfathomable sum, should
come as no surprise, because that is precisely, exactly what
Google was for the last generation of AIs Moreover, it's a pattern of thought. OpenAI did not seek to
simplify the internet. They wrapped it in a very simple surface.
That text box, they took the internet can of worms, 
and they put the can around it And that mode of wrapping complexity 
is what we need in healthcare. I’ll give some examples.
My friend David. He's a primary care physician, one of the most mission centred and
hard working people I've ever met. He used to see between 20 and 30 
patients a day in his practice and then he would go to his computer, 
and write notes, about all of those interactions, into the electronic medical record. Those notes feed the leviathan
of healthcare Administration. All of the revenue cycle, 
insurance claims, billing, government compliance, regulation, legal disputes, quality,
everything comes from those notes. Now, David has an AI. 
It’s on his phone and sits on the table while he's consulting
with his patients. It listens to their interaction and
then writes the notes for him. A machine that listens and writes. What it does is it wraps that whole
administrative complexity for him in an interface so simple 
it seems invisible. Aided by this AI, David now sees between
60 and 70 patients a day, an unimaginable increase in his
productivity and decrease in the cost that his patients experience. Now, David may be Superman,
but he's only one man. Can we apply the same logic to clinical
teams or whole hospitals? You know, a hospital is about the most
profound place in modern American life. Most of us came into this
life in a hospital, and most of us will depart from one. And with all of the 
ambulances and helicopters arriving with gunshot
victims and trauma victims, babies being born, 
hearts being transplanted. The complexity is overwhelming. It's more than anybody can
hold in their mind, but it’s not more than an AI can hold. Imagine an AI wrapping the hospital, 
aware of the status of all of the patients, their journey,
and inferring the next best actions for all the clinicians and workers
in the hospital, and sharing it with them in their language. Coordinating that care, wrapping that
hospital administration for the providers will dramatically
reduce their complexity and increase their quality
and productivity. A small example of this is the sepsis
work I described earlier. HCA brought together teams of
clinicians and engineers, and I was privileged to be one of them. Together, we wrapped the subtle complexity
of sepsis development in an algorithm and a workflow, giving clinicians hours of advance
warning of sepsis. And with that tool, they're able to
save thousands of lives a year. There was a lifetime privilege 
to be on those teams Machines that can read and write
give us new opportunities Consider the case of Martha mills. She was a British teenager who,
on a vacation with her family, slipped on her bicycle and jabbed herself
in the abdomen with the handlebars. The pain was intense. She went to
the E.R. it didn't get better, and she was admitted. 
She deteriorated over the weekend, and ultimately succumbed to death. It was a tragedy, an avoidable
tragedy because the signs and symptoms of sepsis were
all over that chart. But they were missed in the clinical and all too human complexity
of modern medicine. Now imagine an AI that has read
the medical textbooks, has read the clinical literature, and is connected to those charts
in real time and can read them. Such an AI would know about sepsis
and would not miss it. Most excitingly, imagine that
AI exists outside of the traditional healthcare system, allied
and aligned to Martha's family, who were at her bedside disempowered
through this whole experience. Such an AI could have told them Martha
is critically deathly ill and needs to be in a pediatric
ICU right now. Here's why. Armed with that information, her parents would have advocated for her
transfer and probably life saving care. All of that technology 
is available today. So what ails us 
in healthcare, is complexity. And the bitter irony, as I said, is that every attempt we make to improve
matters with better tools, with better therapy, with better drugs, makes things worse 
by adding to complexity. We need to simplify. We can't simplify by standardizing.
So we must simplify by wrapping. And I’ve shown how we can wrap
administration for a single physician, hospital operations for 
clinical teams in hospitals. But most of all, I want you 
to leave knowing that AI can wrap the experience of healthcare for
you and for me as consumers, empowering us as advocates, knowers and deciders in our care,
and in the care of those we love. Thank you 




## Project idea

Analyzing different types of medical images to assist doctors in detecting and diagnosing diseases. **AI-powered medical image diagnosis system** that analyzes **X-rays, MRIs, CT scans, and skin images** to detect diseases. Using **deep learning (CNNs)** and **computer vision**, it provides real-time diagnostic insights with heatmaps for explainability. The system features a **user-friendly web interface** and integrates with hospital **EHR systems** to enhance workflow efficiency. By improving accuracy and reducing diagnostic time, MediScan AI supports doctors in **early disease detection and clinical decision-making**.

## Use Cases

1.  **Radiology (X-rays & CT Scans) – Detecting Bone & Chest Diseases**
	- Detect fractures, pneumonia, tuberculosis, and lung diseases.
	- Identify bone abnormalities (osteoporosis, arthritis).
**Datasets:** 
	  Chest x-ray: https://paperswithcode.com/dataset/chestx-ray14
	  Pneumonia dataset: https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge
	  MURA Dataset for bone fracture: https://stanfordmlgroup.github.io/competitions/mura/

1. **Dermatology (Skin Images) – Detecting Skin Diseases**
	 Dermoscopy images (high-resolution images of skin)  
	**Use Cases:**  
	- Detect **skin cancer (melanoma, carcinoma), infections, rashes, or acne**.  
	- Detect common skin conditions such as scabies, etc.
	- Useful for **early detection of malignant melanoma**.
	
	 **Datasets:**  
	 ISIC 2020 Dataset – https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge
	 Derm7pt Dataset – https://github.com/jeremykawahara/derm7pt
	 HAM10000 – https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000

2.  **Cardiology (Echocardiography & ECG Images) – Detecting Heart Diseases**

- **Images to Analyze:** Echocardiograms, Electrocardiograms (ECG)  
 **Use Cases:**  
- Detect **heart valve diseases, arrhythmias, and cardiomyopathy**.  
- AI models help in **automated interpretation of heart conditions**.

**Dataset**
 PhysioNet ECG Dataset – ECG signals with labeled cardiac diseases. https://physionet.org/content/?topic=ecg  
 EchoNet-Dynamic – Echocardiography dataset for heart function analysis.  
 MIT-BIH Arrhythmia Database – https://www.physionet.org/content/mitdb/1.0.0/
